{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DOWNLOAD DATA AND UNZIP IT"
      ],
      "metadata": {
        "id": "Gk7h_dyC0wpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data from https://voxellab.pl/EmoNeuroDB/\n",
        "\n",
        "# emoneuro_challenge/data/raw contains the original data provided by the competition. That is our start point."
      ],
      "metadata": {
        "id": "cQhwuSRVdxgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORT LIBRARIES"
      ],
      "metadata": {
        "id": "UU92bMz10_Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import signal\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pickle\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from sklearn.decomposition import PCA\n",
        "import gc\n",
        "from keras import backend as K\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "URe5s3rM6LMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODULE 1: Data Pre-processing"
      ],
      "metadata": {
        "id": "CUFl5gvg1IUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocess_data():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def Preprocess_Signal(self, df):\n",
        "        # Define the channel names (some of them are discarded, i.e., X1, X2...\n",
        "        # not found at https://upload.wikimedia.org/wikipedia/commons/thumb/7/70/21_electrodes_of_International_10-20_system_for_EEG.svg/1200px-21_electrodes_of_International_10-20_system_for_EEG.svg.png)\n",
        "        #subset:\n",
        "        channel_names = ['Fp1', 'Fp2', 'F7', 'F8', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2',\n",
        "                        'F3', 'F4', 'T3', 'T4', 'T5', 'T6', 'Fz', 'Cz', 'A1', 'A2']\n",
        "        #All:\n",
        "        #channel_names = ['P3', 'C3', 'F3', 'Fz', 'F4', 'C4', 'P4', 'Cz', 'CM', 'A1',\n",
        "        #                 'Fp1', 'Fp2', 'T3', 'T5', 'O1', 'O2', 'X3', 'X2', 'F7', 'F8', 'X1',\n",
        "        #                 'A2', 'T6', 'T4']\n",
        "        # Create a copy of the DataFrame for processing\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Specify the reference location\n",
        "        mastoid_channels = ['A1', 'A2']\n",
        "\n",
        "        sample_freq = 300  # Sample frequency (Hz)\n",
        "\n",
        "        # Re-reference to mastoid_channels\n",
        "        for sensor in channel_names:\n",
        "            df_processed[sensor] = df[sensor] - df[mastoid_channels].mean(axis=1)\n",
        "\n",
        "\n",
        "        # Define filter parameters\n",
        "        hp_freq = 1  # High-pass filter cutoff frequency (Hz)\n",
        "        lp_freq = 50  # Low-pass filter cutoff frequency (Hz)\n",
        "        filter_delay_ms = 40  # Filter delay (ms)\n",
        "        sample_freq = 300  # Sample frequency (Hz)\n",
        "\n",
        "        # Design high-pass and low-pass filters\n",
        "        b_hp, a_hp = signal.butter(N=4, Wn=hp_freq / (sample_freq / 2), btype='high', analog=False)\n",
        "        b_lp, a_lp = signal.butter(N=4, Wn=lp_freq / (sample_freq / 2), btype='low', analog=False)\n",
        "\n",
        "        # Apply high-pass filter to each sensor's data\n",
        "        df_filtered_hp = df_processed.apply(lambda col: signal.filtfilt(b_hp, a_hp, col))\n",
        "\n",
        "        # Apply low-pass filter to each sensor's data\n",
        "        df_filtered = df_filtered_hp.apply(lambda col: signal.filtfilt(b_lp, a_lp, col))\n",
        "\n",
        "        # Remove filter delay\n",
        "        filter_delay_samples = int(filter_delay_ms * sample_freq / 1000)\n",
        "        df_filtered_delay_removed = df_filtered.iloc[filter_delay_samples:]\n",
        "\n",
        "        # Perform FFT on each sensor's data\n",
        "        fft_results = {}\n",
        "        for col in df.columns:\n",
        "            fft_result = np.fft.fft(df_filtered_delay_removed[col])\n",
        "            frequencies = np.fft.fftfreq(len(fft_result), d=1/sample_freq)\n",
        "            fft_results[col] = (frequencies, np.abs(fft_result))\n",
        "        return fft_results\n",
        "\n",
        "\n",
        "\n",
        "    def generate_train_data(self, train_data_path, train_labels_file):\n",
        "\n",
        "        labels = pd.read_csv(train_labels_file)\n",
        "\n",
        "        files = os.listdir(train_data_path)\n",
        "        # Sort files alphabetically\n",
        "        files.sort()\n",
        "\n",
        "        df_tot = []\n",
        "        # Process files in alphabetical order\n",
        "        for file_name in tqdm(files):\n",
        "            file_path = os.path.join(train_data_path, file_name)\n",
        "\n",
        "            #print(file_path)\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            res = self.Preprocess_Signal(df)\n",
        "\n",
        "            new_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "            if 'Time' in res.keys():\n",
        "                new_df['Time'] = df.Time.values[:len(res['Time'][0])]\n",
        "\n",
        "            for channel in res.keys():\n",
        "                new_df[channel+'_Amp'] = res[channel][1]\n",
        "\n",
        "            new_df.drop(['Time_Amp'], axis=1, inplace=True)\n",
        "            new_df['label'] = labels[labels.filename==file_name]['class'].values[0]\n",
        "            new_df['filename'] = file_name.split('.')[0]\n",
        "\n",
        "            if len(df_tot)==0:\n",
        "                df_tot = new_df\n",
        "            else:\n",
        "                df_tot = pd.concat([df_tot, new_df], ignore_index=True)\n",
        "\n",
        "        return df_tot\n",
        "\n",
        "    def generate_test_data(self, data_dir):\n",
        "        files = os.listdir(data_dir)\n",
        "        files.sort()\n",
        "\n",
        "        df_tot = []\n",
        "\n",
        "        # List all CSV files in the directory\n",
        "        csv_files = [f for f in files if f.endswith('.csv')]\n",
        "        # Load and evaluate each DataFrame\n",
        "        dataframe_list = []\n",
        "        for csv_file in tqdm(csv_files):\n",
        "            #print(csv_file)\n",
        "            file_path = os.path.join(data_dir, csv_file)\n",
        "            df = pd.read_csv(file_path)\n",
        "            #new_df = df\n",
        "            new_df = pd.DataFrame()\n",
        "\n",
        "            res = self.Preprocess_Signal(df)\n",
        "\n",
        "            if 'Time' in res.keys():\n",
        "                new_df['Time'] = df.Time.values[:len(res['Time'][0])]\n",
        "\n",
        "            for channel in res.keys():\n",
        "                new_df[channel+'_Amp'] = res[channel][1]\n",
        "\n",
        "            new_df['filename'] = csv_file.split('.')[0]\n",
        "\n",
        "            new_df.drop(['Time_Amp'], axis=1, inplace=True)\n",
        "\n",
        "            if len(df_tot)==0:\n",
        "                df_tot = new_df\n",
        "            else:\n",
        "                df_tot = pd.concat([df_tot, new_df], ignore_index=True)\n",
        "        return df_tot\n"
      ],
      "metadata": {
        "id": "B8u0KGD66QZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppd = Preprocess_data()\n",
        "\n",
        "\n",
        "train_data_path = 'emoneuro_challenge/data/raw/train_raw_data/train/'\n",
        "train_labels_file = 'emoneuro_challenge/data/raw/train_raw_data/train_labels.csv'\n",
        "\n",
        "print('Processing training files')\n",
        "df_train = ppd.generate_train_data(train_data_path, train_labels_file)\n",
        "df_train['train'] = 1 # train\n",
        "\n",
        "print('Processing validation files')\n",
        "valid_data_path = 'emoneuro_challenge/data/raw/valid_raw_data/validation'\n",
        "valida_labels_file = 'emoneuro_challenge/data/raw/valid_raw_data/validation_labels.csv'\n",
        "\n",
        "df_val = ppd.generate_train_data(valid_data_path, valida_labels_file)\n",
        "df_val['train'] = 2 # validation\n",
        "\n",
        "print('Processing test files')\n",
        "test_data_path = 'emoneuro_challenge/data/raw/test_raw_data'\n",
        "df_test = ppd.generate_test_data(test_data_path)\n",
        "df_test['train'] = 0\n",
        "df_test['label'] = -1 # dummy label\n",
        "\n",
        "df_train_val = pd.concat([df_train, df_val], ignore_index=True)"
      ],
      "metadata": {
        "id": "Y6jCBIPl6Wvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This may take some time, only if you are interested in saving intermediate data.\n",
        "\n",
        "#outpath = 'emoneuro_challenge/data/processed/stage_1/'\n",
        "\n",
        "#if not os.path.exists(outpath):\n",
        "#    os.makedirs(outpath)\n",
        "\n",
        "#df_train_val.to_csv(os.path.join(outpath, 'train_val.csv'), index=False)\n",
        "#df_test.to_csv(os.path.join(outpath, 'test.csv'), index=False)"
      ],
      "metadata": {
        "id": "Bww6ZNuc6dvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODULE 2: NN INPUT GENERATED\n"
      ],
      "metadata": {
        "id": "jA-hM9ss1FnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generate_Datasets():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def extract_features(self, df, bucket_div, test=0):\n",
        "        left_chn = ['Fp1', 'F7', 'C3', 'P3', 'O1',\n",
        "                                'F3', 'T3', 'T5', 'Fz', 'Cz', 'A1']\n",
        "        right_chn = ['Fp2', 'F8', 'C4', 'P4', 'O2',\n",
        "                                 'F4', 'T4', 'T6', 'Fz', 'Cz', 'A2']\n",
        "        left_nn_chn = [i+'_Amp' for i in left_chn]\n",
        "        right_nn_chn = [i+'_Amp' for i in right_chn]\n",
        "        # Extract video_ID for unique identification\n",
        "        video_IDs = df['filename'].unique()\n",
        "\n",
        "        # Split dataset into features (X) and labels (y) for each video_ID\n",
        "        X_left = []\n",
        "        X_right = []\n",
        "        y = []\n",
        "\n",
        "        for video_ID in video_IDs:\n",
        "            video_data = df[df['filename'] == video_ID]\n",
        "\n",
        "            # Assuming the 'label' column is the same for the entire sequence\n",
        "            label = video_data['label'].iloc[0]\n",
        "\n",
        "            # Drop non-essential columns\n",
        "            video_data = video_data.drop(['filename', 'label', 'user_ID', 'train'], axis=1)\n",
        "\n",
        "            left_data = video_data[left_nn_chn]\n",
        "            right_data = video_data[right_nn_chn]\n",
        "\n",
        "            if bucket_div > 1:\n",
        "                numeric_cols = video_data.select_dtypes(include=[np.number]).columns\n",
        "                string_cols = video_data.select_dtypes(exclude=[np.number]).columns\n",
        "                df_averaged_num = video_data[numeric_cols].groupby(np.arange(len(video_data)) // bucket_div).mean()\n",
        "                df_averaged_str = video_data[string_cols].groupby(np.arange(len(video_data)) // bucket_div).first()\n",
        "                video_data = pd.concat([df_averaged_num, df_averaged_str], axis=1)\n",
        "\n",
        "\n",
        "            # Convert the sequence to a numpy array\n",
        "            sequence_array = left_data.to_numpy()\n",
        "            # Append the sequence and label to X and y\n",
        "            X_left.append(sequence_array)\n",
        "\n",
        "            # Convert the sequence to a numpy array\n",
        "            sequence_array = right_data.to_numpy()\n",
        "            # Append the sequence and label to X and y\n",
        "            X_right.append(sequence_array)\n",
        "\n",
        "            y.append(label)\n",
        "\n",
        "        X_left = np.array(X_left)\n",
        "        X_right = np.array(X_right)\n",
        "        y = np.array(y)\n",
        "        if test == 0:\n",
        "            y = to_categorical(y)\n",
        "        return X_left, X_right, y\n",
        "\n",
        "\n",
        "    def extract_features_train(self, df, train_IDs, val_IDs, bucket_div, scale=0, pca=0):\n",
        "        print('Starting training data massage...')\n",
        "\n",
        "        setOI = df[df['user_ID'].isin(train_IDs+val_IDs)].reset_index(drop=True)\n",
        "        if scale: #fit_transform (train, )\n",
        "            print('Scaling data...')\n",
        "            setOI_clean = setOI.drop(['filename', 'label', 'user_ID', 'train'], axis=1)\n",
        "            scaler = StandardScaler()\n",
        "            setOI_scaled = pd.DataFrame(scaler.fit_transform(setOI_clean), columns=setOI_clean.columns)\n",
        "            setOI_scaled = pd.concat([setOI[['filename', 'label', 'user_ID', 'train']], setOI_scaled], axis=1)\n",
        "            setOI = setOI_scaled\n",
        "        else:\n",
        "            scaler = None\n",
        "\n",
        "        if pca:\n",
        "            assert scale==True\n",
        "            print('Applying PCA..')\n",
        "            # Apply PCA with the number of components equal to the original number of features\n",
        "            setOI_clean = setOI.drop(['filename', 'label', 'user_ID', 'train'], axis=1)\n",
        "            pca_ = PCA(n_components=len(setOI_clean.columns))\n",
        "            setOI_pca = pd.DataFrame(pca_.fit_transform(setOI_clean), columns=setOI_clean.columns)\n",
        "            setOI_pca = pd.concat([setOI[['filename', 'label', 'user_ID', 'train']], setOI_pca], axis=1)\n",
        "            setOI = setOI_pca\n",
        "        else:\n",
        "            pca_=None\n",
        "\n",
        "        train_set = setOI[setOI['user_ID'].isin(train_IDs)].reset_index(drop=True)\n",
        "        validation_set = setOI[setOI['user_ID'].isin(val_IDs)].reset_index(drop=True)\n",
        "\n",
        "        X_left_train, X_right_train, y_train = self.extract_features(train_set, bucket_div)\n",
        "        X_left_val, X_right_val, y_val = self.extract_features(validation_set, bucket_div)\n",
        "\n",
        "        return [X_left_train, X_right_train, y_train], [X_left_val, X_right_val, y_val], scaler, pca_\n",
        "\n",
        "    def extract_features_test(self, df, test_IDs, bucket_div, scale=0, pca=0):\n",
        "        print('Starting test data massage...')\n",
        "\n",
        "        setOI = df[df['user_ID'].isin(test_IDs)].reset_index(drop=True)\n",
        "\n",
        "        if scale:\n",
        "            print('Scaling data...')\n",
        "            setOI_clean = setOI.drop(['filename', 'label', 'user_ID', 'train'], axis=1)\n",
        "            setOI_scaled = pd.DataFrame(scale.transform(setOI_clean), columns=setOI_clean.columns)\n",
        "            setOI_scaled = pd.concat([setOI[['filename', 'label', 'user_ID', 'train']], setOI_scaled], axis=1)\n",
        "            setOI = setOI_scaled\n",
        "            if pca:\n",
        "                print('Applying PCA..')\n",
        "                setOI_clean = setOI.drop(['filename', 'label', 'user_ID', 'train'], axis=1)\n",
        "                setOI_pca = pd.DataFrame(pca.transform(setOI_clean), columns=setOI_clean.columns)\n",
        "                setOI_pca = pd.concat([setOI[['filename', 'label', 'user_ID', 'train']], setOI_pca], axis=1)\n",
        "                setOI = setOI_pca\n",
        "\n",
        "        X_left_test, X_right_test, y_test = self.extract_features(setOI, bucket_div, test=1) #ojo test=0 si meto val para pruebas\n",
        "\n",
        "        return [X_left_test, X_right_test, y_test]\n",
        "\n",
        "\n",
        "\n",
        "    def generate_sets(self, df_train_val, df_test, outpath, scale_data, pca_data, bucket_div):\n",
        "\n",
        "\n",
        "        df_train_val['user_ID'] = df_train_val.filename.str.split('_').str[0]\n",
        "\n",
        "        df_train_ = df_train_val[df_train_val.train==1].copy()\n",
        "        #df_train_['user_ID'] = df_train_.filename.str.split('_').str[0]\n",
        "        #print(df_train_.shape)\n",
        "        train_IDs = list(df_train_.user_ID.unique())\n",
        "\n",
        "        df_val_ = df_train_val[df_train_val.train==2].copy()\n",
        "        #df_val_['user_ID'] = df_val_.filename.str.split('_').str[0]\n",
        "        #print(df_val_.shape)\n",
        "        val_IDs = list(df_val_.user_ID.unique())\n",
        "\n",
        "        df_test_ = df_test.copy()\n",
        "        df_test_['user_ID'] = df_test_.filename.str.split('_').str[0]\n",
        "        #print(df_test_.shape)\n",
        "        test_IDs = list(df_test_.user_ID.unique())\n",
        "        df_test_.filename = df_test_.filename + '.csv'\n",
        "        test_filenames = list(df_test_.filename.unique())\n",
        "\n",
        "        #Training\n",
        "        train, val, scaler, pca = self.extract_features_train(df_train_val.copy(),\n",
        "                                                              train_IDs,\n",
        "                                                              val_IDs,\n",
        "                                                              bucket_div,\n",
        "                                                              scale=scale_data,\n",
        "                                                              pca=pca_data)\n",
        "\n",
        "        test = self.extract_features_test(df_test_, test_IDs, bucket_div, scale=scaler, pca=pca)\n",
        "\n",
        "\n",
        "\n",
        "        filename = os.path.join(outpath, 'dataset_'+str(bucket_div)+'buckets.pkl')\n",
        "        with open(filename, 'wb') as f:\n",
        "            data = {'train': train,\n",
        "                    'val': val,\n",
        "                    'test': test,\n",
        "                    'pca_data': pca_data,\n",
        "                    'scale_data': scale_data,\n",
        "                    'bucket_div': bucket_div,\n",
        "                    'train_IDs': train_IDs,\n",
        "                    'val_IDs': val_IDs,\n",
        "                    'test_IDs': test_IDs,\n",
        "                    'test_filenames': test_filenames\n",
        "                    }\n",
        "            pickle.dump(data, f)"
      ],
      "metadata": {
        "id": "ZMcp2V5r6od6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scale_data=True\n",
        "pca_data = False\n",
        "\n",
        "gd = Generate_Datasets()\n",
        "\n",
        "bucket_divs = [1]#, 2]\n",
        "\n",
        "outpath = 'emoneuro_challenge/data/processed/stage_2'\n",
        "if not os.path.exists(outpath):\n",
        "    os.makedirs(outpath)\n",
        "\n",
        "df_train_val = df_train_val.drop(['Time'], axis=1)\n",
        "df_test = df_test.drop(['Time'], axis=1)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df_train_val['label'] = label_encoder.fit_transform(df_train_val['label'])\n",
        "joblib.dump(label_encoder, 'label_encoder.joblib')\n",
        "\n",
        "\n",
        "for bucket_div in bucket_divs:\n",
        "    gd.generate_sets(df_train_val, df_test, outpath, scale_data, pca_data, bucket_div)"
      ],
      "metadata": {
        "id": "ztVBnvFy98BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODULE 3: CLASSIFICATION"
      ],
      "metadata": {
        "id": "-Y_yQGAl1TT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LSTM_2branches(input_data):\n",
        "\n",
        "    input_layers = []\n",
        "    processed_outputs = []\n",
        "\n",
        "    inp = Input(shape=(input_data.shape[1], input_data.shape[2]), name='input_signal_left')\n",
        "    input_layers.append(inp)\n",
        "    x = Conv1D(filters=32, kernel_size=3, activation='relu')(inp)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Reshape((x.shape[1], x.shape[2]))(x)\n",
        "    x = LSTM(64, return_sequences=True)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    processed_outputs.append(x)\n",
        "\n",
        "    inp2 = Input(shape=(input_data.shape[1], input_data.shape[2]), name='input_signal_right')\n",
        "    input_layers.append(inp2)\n",
        "    x = Conv1D(filters=32, kernel_size=3, activation='relu')(inp2)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Reshape((x.shape[1], x.shape[2]))(x)\n",
        "    x = LSTM(64, return_sequences=True)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    processed_outputs.append(x)\n",
        "    concatenated_out = Concatenate()(processed_outputs)\n",
        "\n",
        "    x = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(concatenated_out)\n",
        "    x = Dense(6, activation='softmax')(x)\n",
        "\n",
        "\n",
        "    model = Model(inputs=input_layers, outputs=x)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "69bnM8iC6p0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier:\n",
        "    def __init__(self, id_experiment):\n",
        "        self.id_experiment = id_experiment\n",
        "\n",
        "\n",
        "    def evaluate_challenge_model(self, id_model, train, val, bucket_div):\n",
        "        print('Starting training...')\n",
        "\n",
        "        X_train_left = train[0]\n",
        "        X_train_right = train[1]\n",
        "        y_train = train[2]\n",
        "        X_val_left = val[0]\n",
        "        X_val_right = val[1]\n",
        "        y_val = val[2]\n",
        "\n",
        "        #LSTM_model.get_model_from_id(id_model, X_train)\n",
        "        model = LSTM_2branches(X_train_left)\n",
        "        #model = Attention_model.get_model_from_id(id_model, X_train)\n",
        "\n",
        "        # Display the model summary\n",
        "        model.summary()\n",
        "        weights_file = 'emoneuro_challenge/weights'\n",
        "        if not os.path.exists(weights_file):\n",
        "            os.makedirs(weights_file)\n",
        "\n",
        "        weights_filename = os.path.join(weights_file, self.id_experiment+ '_'+id_model+'_bckt_'+str(bucket_div)+'_weights.h5')\n",
        "\n",
        "        ckpt = ModelCheckpoint(weights_filename,\n",
        "                            save_best_only=True, save_weights_only=True,\n",
        "                            monitor='val_accuracy', verbose=0, mode='max')\n",
        "\n",
        "        earlystopper = EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0.00001)\n",
        "\n",
        "        # Model fitting\n",
        "        model.fit([X_train_left, X_train_right], y_train, batch_size=64, validation_data=([X_val_left, X_val_right], y_val),\n",
        "            epochs=200, verbose=1,\n",
        "            callbacks=[ckpt, earlystopper, reduce_lr])\n",
        "\n",
        "        # Load the best model\n",
        "        model.load_weights(weights_filename)\n",
        "\n",
        "        print('Evaluating best on train data...')\n",
        "        eval_train = model.evaluate([X_train_left, X_train_right], y_train)\n",
        "\n",
        "        print('Evaluating best on val data...')\n",
        "        eval_train = model.evaluate([X_val_left, X_val_right], y_val)\n",
        "\n",
        "        return eval_train, model\n",
        "\n",
        "    def run_restored_model(self, id_model, weight_file, test, test_filenames, bucket_div):\n",
        "        print('Starting restoring...')\n",
        "\n",
        "        X_test = test\n",
        "\n",
        "\n",
        "        model = LSTM_2branches(X_test[0])\n",
        "\n",
        "        # Load the best model\n",
        "        model.load_weights(weights_filename)\n",
        "\n",
        "\n",
        "        # Display the model summary\n",
        "        model.summary()\n",
        "\n",
        "        label_encoder = joblib.load('label_encoder.joblib')\n",
        "\n",
        "        eval_path = 'emoneuro_challenge/evaluation_from_saved_model'\n",
        "        if not os.path.exists(eval_path):\n",
        "            os.makedirs(eval_path)\n",
        "\n",
        "        # Use the evaluate_dataframes function\n",
        "        print('Evaluating dataframes...')\n",
        "\n",
        "        # Make predictions on the new data\n",
        "        predictions = model.predict([X_test[0], X_test[1]])\n",
        "\n",
        "        # Convert predicted class indices back to string labels\n",
        "        predicted_classes = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "\n",
        "        # Create a DataFrame with 'video_ID' and 'predicted_class'\n",
        "        result_df = pd.DataFrame({'filename': test_filenames, 'class': predicted_classes})\n",
        "\n",
        "        print(result_df['class'].value_counts())\n",
        "        result_df.to_csv(os.path.join(eval_path, 'eval_'+id_model+'bckt_'+str(bucket_div)+'.csv'), index=False)\n",
        "\n",
        "\n",
        "        del model\n",
        "        K.clear_session()\n",
        "        gc.collect()\n",
        "\n",
        "    def generate_evaluation_file(self, id_test, X_test, test_filenames, best_model, bucket_div):\n",
        "\n",
        "        label_encoder = joblib.load('label_encoder.joblib')\n",
        "\n",
        "        eval_path = 'emoneuro_challenge/evaluation_from_trained_model'\n",
        "        if not os.path.exists(eval_path):\n",
        "            os.makedirs(eval_path)\n",
        "\n",
        "        # Use the evaluate_dataframes function\n",
        "        print('Evaluating dataframes...')\n",
        "\n",
        "        # Make predictions on the new data\n",
        "        predictions = best_model.predict([X_test[0], X_test[1]])\n",
        "\n",
        "        # Convert predicted class indices back to string labels\n",
        "        predicted_classes = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
        "\n",
        "        # Create a DataFrame with 'video_ID' and 'predicted_class'\n",
        "        result_df = pd.DataFrame({'filename': test_filenames, 'class': predicted_classes})\n",
        "\n",
        "        print(result_df['class'].value_counts())\n",
        "\n",
        "        result_df.to_csv(os.path.join(eval_path, id_test+'bckt_'+str(bucket_div)+'.csv'), index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "is-EY-AU6p4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a pre-trained solution\n",
        "\n",
        "datapath = 'emoneuro_challenge/data/processed/stage_2/dataset_1buckets.pkl'\n",
        "with open(datapath, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "\n",
        "\n",
        "experiment_id = 'Dev_LSTM'\n",
        "\n",
        "\n",
        "clf = Classifier(experiment_id)\n",
        "\n",
        "model_id = 'model_1_2branch'\n",
        "\n",
        "\n",
        "train = data['train']\n",
        "val = data['val']\n",
        "test = data['test']\n",
        "pca_data=data['pca_data']\n",
        "bucket_div = data['bucket_div']\n",
        "train_IDs = data['train_IDs']\n",
        "val_IDs = data['val_IDs']\n",
        "test_IDs = data['test_IDs']\n",
        "test_filenames = data['test_filenames']\n",
        "\n",
        "\n",
        "print('2 branches with {0} Buckets'.format(bucket_div))\n",
        "print('Train set: {0}'.format(train_IDs))\n",
        "print('Val set: {0}'.format(val_IDs))\n",
        "print('Test set: {0}'.format(test_IDs))\n",
        "\n",
        "print('Model: ', model_id)\n",
        "\n",
        "weights_filename = os.path.join('emoneuro_challenge/weights', 'Dev_LSTM_2branches.h5')\n",
        "clf.run_restored_model(model_id, weights_filename, test, test_filenames, bucket_div)\n"
      ],
      "metadata": {
        "id": "gyiRcrVd6p7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you are interested in training the model\n",
        "\n",
        "datapath = 'emoneuro_challenge/data/processed/stage_2/dataset_1buckets.pkl'\n",
        "with open(datapath, 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "train_dict = {}\n",
        "train_dict['Experiment']=[]\n",
        "train_dict['Test']=[]\n",
        "train_dict['Train']=[]\n",
        "train_dict['Val']=[]\n",
        "train_dict['Val_Acc']=[]\n",
        "train_dict['Val_Loss']=[]\n",
        "train_dict['Test_Acc']=[]\n",
        "train_dict['Test_Loss']=[]\n",
        "\n",
        "\n",
        "test_dict = {}\n",
        "test_dict['Experiment']=[]\n",
        "test_dict['Train']=[]\n",
        "test_dict['Val']=[]\n",
        "test_dict['Val_Acc']=[]\n",
        "test_dict['Val_Loss']=[]\n",
        "\n",
        "experiment_id = 'Trained_LSTM_2b'\n",
        "\n",
        "clf = Classifier(experiment_id)\n",
        "\n",
        "model_id = 'model_1_2branch'\n",
        "\n",
        "\n",
        "train = data['train']\n",
        "val = data['val']\n",
        "test = data['test']\n",
        "bucket_div = data['bucket_div']\n",
        "train_IDs = data['train_IDs']\n",
        "val_IDs = data['val_IDs']\n",
        "test_IDs = data['test_IDs']\n",
        "test_filenames = data['test_filenames']\n",
        "\n",
        "\n",
        "print('{0} Buckets'.format(bucket_div))\n",
        "print('Train set: {0}'.format(train_IDs))\n",
        "print('Val set: {0}'.format(val_IDs))\n",
        "print('Test set: {0}'.format(test_IDs))\n",
        "\n",
        "eval_train, best_model = clf.evaluate_challenge_model(model_id, train, val, bucket_div)\n",
        "res_df = clf.generate_evaluation_file(model_id, test, test_filenames, best_model, bucket_div)"
      ],
      "metadata": {
        "id": "tBYRGIVtobFj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}